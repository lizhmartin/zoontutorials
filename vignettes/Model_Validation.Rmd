---
title: "Model Evaluation"
output:
  html_document:
    css: zoon.css
    theme: lumen
    toc: yes
    toc_float:
      collapsed: no
      toc_depth: 4
  pdf_document:
    toc: yes
vignette: |
  %\VignetteIndexEntry{Model Validation}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r knitr_options, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
# set up knitr options
knitr::opts_chunk$set(message = FALSE,
               warning = FALSE,
               fig.align = 'center',
               dev = c('png'),
               cache = TRUE)
```

<hr>

# Introduction

Species distribution models are used for all sorts of important decisions: choosing protected areas, prioritising conservation actions, informing development applications, even determining the official conservation status of species. As such, we need to know how much to believe our models. We need to know if they are up to the task we set for them. We do that by evaluating our model's performance.

Model evaluation is the process of determining if our model is close enough to reality for our purpose. Models are smaller, simpler, hopefully useful versions of reality, but all models are wrong. Whether our imperfect model is useful is dependent on what we're going to use it for. For example, if we wish to predict the geographic expansion of a species under a changing climate, we need to know if our model can extrapolate across geographic space. But, if we wish only to accurately map the current distribution of a species, we don't mind much at all if our model can't extrapolate accurately. It is thus important to consider not how good our model is in absolute terms, but  rather if our model is *good enough* for its purpose. Model evaluation, then, is the process of checking our model outputs to determine if they are a close enough representation of reality for the purpose we're using it for. Are our models fit-for-purpose?

In this `zoon` guide, we will discuss different methods of evaluating a model (or set of models), when we might choose one method over another, and how to use `zoon` to implement these different evaluation methods. We split model evaluation methods and metrics into three broad categories increasing in thoroughness: the bare minimum, internal evaluation, and cross-validation.

Throughout this guide, we'll work with thre Carolina wren. Here is our `zoon` workflow saved as a zoon object:

```{r Library, message=FALSE, warning=FALSE}
library(zoon)
```

```{r workflow, message=FALSE, warning=FALSE, fig.align='center', fig.height=7, fig.width=7, cache = TRUE}
wf <- workflow(occurrence = CarolinaWrenPA,
               covariate = CarolinaWrenRasters,
               process = NoProcess,
               model = LogisticRegression,
               output = NoOutput)
```

# The bare minimum

The Bare Minimum is what we absolutely must do in order to be able to defend our model. The bare minimum is about checking that our results *make sense*. Does our model make sense given the data we fit it with and does it make sense when considering our internal model of the system we're studying.

First things first, let's visually check the raw data against the fitted result using a map. We'll plot the predicted surface of our model using the `PrintMap` ourput module and overlay the raw date with the `points=TRUE` argument.



```{r map, message=FALSE, warning=FALSE, fig.align='center', fig.height=7, fig.width=7, cache = TRUE}

ChangeWorkflow(wf, output = PrintMap(points=TRUE))
```

By exploring the map, we can check that the absences (or background points) overlay areas of low probability and the presences overlay areas of high probability, broadly. In the map of the carolina wren logistic regression SDM, we can see that the area with high probability in the southeast are overlain by the presences points and the purple areas, low probabilty, are overlain with the absences points.

We can also use our ecological knowledge - our internal model of the system - to question the model output. We expect the Carolina wren to be associated with X type of environment which we know is present in the southeast of the US and absent in the north and west. Our predicted distribution matches our expectations.

This is really the bare minimum we can do - check that our statistical model matches our internal model, or rather, current knowledge about the system. That isn't to say that we should be loyal to what we already know - part of building models is to further scientific understanding. Indeed by exploring th epredicted distribution in the context of current understanding, new gaps in knowledge are often identified. These gaps can be further explored in future studies.

Next we might check that the shape of the relationships to see that they match our understanding of the system. We can plot the response curves using `ResponsePlot`

```{r response plots, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=4}

ChangeWorkflow(wf, output = ResponsePlot)
```

From these response plots, we see that the probability of occurrence decreases with increasing latitude (toward the equator), which matches our expectation given that presences increase moving eastwards.

Lastly, in our bare minimum model evalaution, we can check the coeffcients of our linear predicted (for parametric models). When we're checking the coefficients, we're makign sure that the sign of the coefficeint matches the raw data. It would be spurious if, after plotting the raw data, that a positive relationship was predicted when a negative one is observed. We can check coefficients by using the output module ` `.

```{r coefficients, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=4}

ChangeWorkflow(wf, output = CoefficientPlot)

```

In this coefficient plot, from our logistic regression SDM, we can see the estimated coefficient for each of our included covariates. The covariates that have a p-value of less than 0.05 are orange and the non-significant coefficeints are navy blue. For the carolina wren...


Let's do all these things at once:
```{r chain bare minimum, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=4}

ChangeWorkflow(wf, output = Chain(PrintMap, ResponsePlot, CoefficientPlot))

```

# Measuring performance

So far we've just eye-balled the goodness of fit. Let's measure it. We do this with performance measures, which in zoon, are located in the `PerformanceMeasures` output module:

```{r performance measures, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=4}

ChangeWorkflow(wf, output = PerformanceMeasures)

```

The performance measures module returns a series of numbers that measure how well the predicted data match the actual data. Of these, the area under the receiver operating characteristic curve (AUC) is the most common. The AUC measures how good a model is at discriminating a site where a species is present from a site where teh pecies is absent. An AUC of 0.5 means the model is no better than random, an AUC of 1 means the model is perfect. This output module also provide a number of other less popular measure of model fit.

When we use the PerformanceMeasures output module we get a warning saying we "have no cross-validation folds, validation statistics may be misleading". This is because model fit should be tested on independent data. When we have no independent data, we can pretend we do by having a training dataset and a test dataset, which is  aprotion of our data that we leave out of the model to test the fit with.

# Cross-validation

```{r CV performance measure, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=4}

ChangeWorkflow(wf,
               process = Crossvalidate(k = 10),
               output = PerformanceMeasures)

```

Or if we just want AUC:

```{r}

ChangeWorkflow(wf,
               process = Crossvalidate(k = 10),
               output = AUC)

```

# Comparing models

We can use the performance measuer discussed above to select between models

```{r compare models, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=4}
two_models <- workflow(occurrence = CarolinaWrenPA,
                          covariate = CarolinaWrenRasters,
                          process = Crossvalidate(k = 10),
                          model = list(LogisticRegression, MaxNet),
                          output = AUC)
```

When we have more than one model module in a list, our performance measures are returned in a list of length n models

```{r performance measure extract compare models, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=4}
Output(two_models)

```







